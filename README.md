# OSDA_hw

## Sternberg Dataset
For the Sternberg dataset EEG-data recorded for the study of the relationship between working memory load and motivation was taken. I took the power of the signal in the theta-frequency band (4-8 Hz) from 64 EEG channels that covered the entire head. The features in the dataset are the power of the signal in each time point of the recording, the target was the number of elements to remember (3 – low working memory load, 15 – high working memory load). The target values were transformed to 0 (3 elements) and 1 (15 elements). The idea was to predict the working memory load based on the EEG-signal. The number of observations in each class was approximately equal.
To binarize the data I have split the dataset into train (70%) and test subsets, scaled it by the standard scaler and divided the resulting signals into 4 bins by the quartile. So, each observation was converted to a number in range of 4, reflecting the number of quartile the signal at this moment belonged to. Further, I used one-hot encoding to transform the data from the 4- to 2-valued context. Additionally, I have dropped the first column of each one-hot-encoded category to avoid milticollinearity.
Next, I fitted 4 baseline models, using 4 cross-validation iterations and grid-search to find the optimal parameters.

## SC_data dataset
For this data I used magnetoencephalography (MEG) recording from the study where people had to read sentences from the screen. I used time periods, when the participants perceived presented words (target value 1) or saw a blank screen (target value 0). I have picked only those channels that record the signal from the occipital cortex (where the primary visual area is located). The idea was to predict the perceived visual information (word or blank space) based on the MEG recording from the occipital cortex. The features were the signals recorded from the particular channels in the prior selected time-window. The number of observations in each class was approximately equal.
To binarize the data I have split the dataset into train (70%) and test subsets, scaled it by the standard scaler and divided the resulting signals into 5 bins by the quartile. So, each observation was converted to a number in range of 5, reflecting the number of quartiles the signal at this moment belonged to. Further, I used one-hot encoding to transform the data from the 4- to 2-valued context. Additionally, I have dropped the first column of each one-hot-encoded category to avoid milticollinearity.
Next, I fitted 4 baseline models, using 4 cross-validation iterations and grid-search to find the optimal parameters.

## aphasia_BCI dataset
For this data EEG recordings of one neurologically healthy person was used. The participant was presented with pictures and had heard corresponding or random words. The person’s task was to match (to oneself, but not out loud) a visually presented picture with a pronounced word. The EEG was recorded from 2 channels only located on the forehead. The idea was to predict the cognitive state (target: 1 – matched a word and a picture correctly; 0 – did not match a word and a picture correctly). Each observation in the data set correspond to a time-interval, when a participant was present a word and a picture (an epoch). Features are time points (the number of which reflects the sampling frequency). Due to a comparable number of features and observations I have additionally conducted a PCA to reduce the matrix rank (from 3500 features to 80).
To binarize the data I have split the dataset into train (70%) and test subsets, scaled it by the standard scaler and divided the resulting signals into 4 bins by the quartile. So, each observation was converted to a number in range of 4, reflecting the number of quartiles the signal at this moment belonged to. Further, I used one-hot encoding to transform the data from the 4- to 2-valued context. Additionally, I have dropped the first column of each one-hot-encoded category to avoid milticollinearity. Following that from the total number of obtained features I have computed the Fisher information score to select only the most relevant features, to decrease computational load and to increase explanatory power of the models.

